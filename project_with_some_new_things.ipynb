{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOyOkJJA7Foy"
   },
   "source": [
    "# Machine Learning in Python - Group Project 1\n",
    "\n",
    "**Due Friday, March 10th by 16.00 pm.**\n",
    "\n",
    "*Luke Appleby, Josep Reyero, Tom Birkbeck, Adam Dunajski*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPpPRE37Fo0"
   },
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jb4qh-pi7Fo1"
   },
   "outputs": [],
   "source": [
    "# Add any additional libraries or submodules below\n",
    "\n",
    "# Data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules that are necessary\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hjs5785u7Fo1"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"the_office.csv\")\n",
    "data_explore = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MLP_cold_opens.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1348\\1091642909.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MLP_cold_opens.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\")\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MLP_cold_opens.txt'"
     ]
    }
   ],
   "source": [
    "with open('MLP_cold_opens.txt', 'r') as file:\n",
    "    new_data = file.read().replace('\\n', '')\n",
    "\n",
    "x = new_data.split(\")\")\n",
    "hots = []\n",
    "for i in range(len(x)):\n",
    "    if x[i][0:4] == 'None':\n",
    "        hots.append(x[i][7:-1])\n",
    "    elif x[i][0:8] == 'Season 1' and x[i][8:12] == 'None':\n",
    "        hots.append(x[i][14:])\n",
    "    elif x[i][0:6] == 'Season' and x[i][8:12] == 'None':\n",
    "        hots.append(x[i][15:-1])\n",
    "\n",
    "hots_df = pd.DataFrame (hots, columns = ['episode_name'])\n",
    "hots_df['cold_open'] = np.zeros(12).astype(int)\n",
    "hots_df = hots_df.replace(to_replace = 'Weight Loss', value = 'Weight Loss (Parts 1&2)')\n",
    "data_ = pd.merge(data, hots_df, on='episode_name', how='left')\n",
    "data_['cold_open'] = data_['cold_open'].fillna(1)\n",
    "\n",
    "data_.loc[data_['cold_open'] == 1]\n",
    "\n",
    "data = data_\n",
    "data_explore = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making sure that all the necessary libraries or submodules are uploaded here, please follow the given skeleton to create your project report. \n",
    "- Your completed assignment must follow this structure \n",
    "- You should not add or remove any of these sections, if you feel it is necessary you may add extra subsections within each (such as *2.1. Encoding*). \n",
    "\n",
    "**Do not forget to remove the instructions for each section in the final document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K314dGEL7Fo1"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "*This section should include a brief introduction to the task and the data (assume this is a report you are delivering to a client).* \n",
    "\n",
    "- If you use any additional data sources, you should introduce them here and discuss why they were included.\n",
    "\n",
    "- Briefly outline the approaches being used and the conclusions that you are able to draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P6Vdzbo7Fo2"
   },
   "source": [
    "## 2. Exploratory Data Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arg9_dYE7Fo2"
   },
   "source": [
    "*Include a detailed discussion of the data with a particular emphasis on the features of the data that are relevant for the subsequent modeling.* \n",
    "\n",
    "- Including visualizations of the data is strongly encouraged - all code and plots must also be described in the write up. \n",
    "- Think carefully about whether each plot needs to be included in your final draft - your report should include figures but they should be as focused and impactful as possible.\n",
    "\n",
    "*Additionally, this section should also implement and describe any preprocessing / feature engineering of the data.*\n",
    "\n",
    "- Specifically, this should be any code that you use to generate new columns in the data frame `d`. All of this processing is explicitly meant to occur before we split the data in to training and testing subsets. \n",
    "- Processing that will be performed as part of an sklearn pipeline can be mentioned here but should be implemented in the following section.*\n",
    "\n",
    "**All code and figures should be accompanied by text that provides an overview / context to what is being done or presented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Columns in the df are:\")\n",
    "print (data_explore.columns.tolist(), \"\\n\")\n",
    "\n",
    "dup_num = pd.DataFrame(data_explore).duplicated().sum()\n",
    "print(\"There are {} duplicated values in the data\".format(dup_num))\n",
    "\n",
    "na_num = data_explore.isna().sum().sum()\n",
    "print(f\"There are {na_num} Na values in the data\")\n",
    "\n",
    "data_explore.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the correlation between the current numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (10, 6)})\n",
    "sns.heatmap(data_explore.corr(), annot = True, fmt = '.2f', linewidths = 2)\n",
    "plt.title(\"Correlation Heatmap for current numerical variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first drop the episode name and episode number features because they are not relevant to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_explore = data_explore.drop(\"episode_name\", axis=1)\n",
    "data_explore = data_explore.drop(\"episode\", axis=1)\n",
    "\n",
    "print (data_explore.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the variable \"air_date\" to create two new features: Categorical variable \"calendar_season\", being the season (autumn, winter, spring, summer) when the episode was aired; and (numerical variable?) \"air_month\" being the month the episode aired. After this we drop \"air_date\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['air_date'] = pd.to_datetime(data['air_date'])\n",
    "\n",
    "# convert the 'dates' column to datetime format\n",
    "data_explore['air_date'] = pd.to_datetime(data_explore['air_date'])\n",
    "\n",
    "# create a new categorical column 'calendar_season' by assigning a category \"winter\", \n",
    "# \"spring\", \"summer\", \"autumn\" depending on the air-month.\n",
    "data_explore['calendar_season'] = pd.cut(\n",
    "    data_explore['air_date'].dt.month, \n",
    "    [0, 3, 6, 9, 12], \n",
    "    labels=['winter', 'spring', 'summer', 'autumn']\n",
    ")\n",
    "\n",
    "# convert calender season to string column rather than categorical column.\n",
    "data_explore['calendar_season'] = data_explore['calendar_season'].astype(str)\n",
    "\n",
    "# Create month column.\n",
    "data_explore['month'] = data_explore['air_date'].dt.month\n",
    "\n",
    "data_explore.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_explore = data_explore.drop(\"air_date\", axis=1)\n",
    "data_explore.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a one hot encoder function to assign a binary attribute per category for the Categorical variables (\"director, \"writer\", \"calendar_season\" and \"main_chars\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHot_encoder(df, cat_variable):\n",
    "    oneHot_df = df.copy()\n",
    "    variable_initials = ''.join([word[0] for word in cat_variable.split(\"_\")])\n",
    "\n",
    "    \n",
    "    unique_chars = set(';'.join(oneHot_df[cat_variable]).split(';'))\n",
    "\n",
    "    # Create binary attributes for each unique character\n",
    "    for char in unique_chars:\n",
    "        oneHot_df[variable_initials+\"_\"+char] = oneHot_df[cat_variable].apply(lambda x: 1 if char in x else 0)\n",
    "    \n",
    "    \n",
    "    # Drop the original main_chars column\n",
    "    oneHot_df = oneHot_df.drop(columns=[cat_variable])\n",
    "    \n",
    "    return oneHot_df\n",
    "\n",
    "\n",
    "cat_variables = ['director', 'writer', 'main_chars', 'calendar_season']\n",
    "\n",
    "for cat_variable in cat_variables:\n",
    "    data_explore = OneHot_encoder(data_explore, cat_variable)\n",
    "    \n",
    "\n",
    "data_explore.head(5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection we realised that there are some repeated directors where in some data entries the name of the director is spelled wrong. The specific cases were:\n",
    "\n",
    " - Charles McDougall (correct name) and Charles McDougal\n",
    " - Claire Scanlon (correct name) and Claire Scanlong \n",
    " - Greg Daniels (correct name) and Greg Daneils\n",
    " - Ken Whittingham (correct name) and Ken Wittingham \n",
    " - Paul Lieberstein (correct name) and Paul Lieerstein\n",
    " \n",
    "Thus we now write code to remove columns with the wrong name, where we will put a 1 in the correct name column if the incorrect one had a 1 and the correct one didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_names = ['d_Charles McDougall','d_Claire Scanlon','d_Greg Daniels', 'd_Ken Whittingham', 'd_Paul Lieberstein']\n",
    "wrong_names = ['d_Charles McDougal','d_Claire Scanlong','d_Greg Daneils', 'd_Ken Wittingham', 'd_Paul Lieerstein']\n",
    "\n",
    "for i, correct_name in enumerate(correct_names):\n",
    "    data_explore.loc[data_explore[wrong_names[i]] == 1, correct_name] = 1\n",
    "    data_explore = data_explore.drop(wrong_names[i], axis=1)\n",
    "    \n",
    "data_explore.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad7J-Yw67Fo3"
   },
   "source": [
    "## 3. Model Fitting and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXdvOaZs7Fo3"
   },
   "source": [
    "*In this section you should detail your choice of model and describe the process used to refine and fit that model.*\n",
    "\n",
    "- You are strongly encouraged to explore many different modeling methods (e.g. linear regression, regression trees, lasso, etc.) but you should not include a detailed narrative of all of these attempts. \n",
    "- At most this section should mention the methods explored and why they were rejected - most of your effort should go into describing the model you are using and your process for tuning and validatin it.\n",
    "\n",
    "*For example if you considered a linear regression model, a classification tree, and a lasso model and ultimately settled on the linear regression approach then you should mention that other two approaches were tried but do not include any of the code or any in depth discussion of these models beyond why they were rejected. This section should then detail is the development of the linear regression model in terms of features used, interactions considered, and any additional tuning and validation which ultimately led to your final model.* \n",
    "\n",
    "**This section should also include the full implementation of your final model, including all necessary validation. As with figures, any included code must also be addressed in the text of the document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edQVULLU7Fo3"
   },
   "source": [
    "## 4. Discussion and Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi0yKPrQ7Fo4"
   },
   "source": [
    "*In this section you should provide a general overview of **your final model**, its **performance**, and **reliability**.* \n",
    "\n",
    "- You should discuss what the implications of your model are in terms of the included features, predictive performance, and anything else you think is relevant.\n",
    "\n",
    "- This should be written with a target audience of a NBC Universal executive who is with the show and university level mathematics but not necessarily someone who has taken a postgraduate statistical modeling course. \n",
    "\n",
    "- Your goal should be to convince this audience that your model is both accurate and useful.\n",
    "\n",
    "- Finally, you should include concrete recommendations on what NBC Universal should do to make their reunion episode a popular as possible.\n",
    "\n",
    "**Keep in mind that a negative result, i.e. a model that does not work well predictively, but that is well explained and justified in terms of why it failed will likely receive higher marks than a model with strong predictive performance but with poor or incorrect explanations / justifications.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References\n",
    "\n",
    "*In this section, you should present a list of external sources (except the course materials) that you used during the project, if any*\n",
    "\n",
    "- Additional data sources can be cited here, in addition to related python documentations, any other webpage sources that you benefited from"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
