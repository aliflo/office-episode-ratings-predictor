{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOyOkJJA7Foy"
   },
   "source": [
    "# Machine Learning in Python - Group Project 1\n",
    "\n",
    "**Due Friday, March 10th by 16.00 pm.**\n",
    "\n",
    "*Luke Appleby, Josep Reyero, Tom Birkbeck, Adam Dunajski*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPpPRE37Fo0"
   },
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jb4qh-pi7Fo1"
   },
   "outputs": [],
   "source": [
    "# Add any additional libraries or submodules below\n",
    "\n",
    "# Data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules that are necessary\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hjs5785u7Fo1"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"the_office.csv\")\n",
    "data_explore = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MLP_cold_opens.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1348\\1091642909.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MLP_cold_opens.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\")\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MLP_cold_opens.txt'"
     ]
    }
   ],
   "source": [
    "with open('MLP_cold_opens.txt', 'r') as file:\n",
    "    new_data = file.read().replace('\\n', '')\n",
    "\n",
    "x = new_data.split(\")\")\n",
    "hots = []\n",
    "for i in range(len(x)):\n",
    "    if x[i][0:4] == 'None':\n",
    "        hots.append(x[i][7:-1])\n",
    "    elif x[i][0:8] == 'Season 1' and x[i][8:12] == 'None':\n",
    "        hots.append(x[i][14:])\n",
    "    elif x[i][0:6] == 'Season' and x[i][8:12] == 'None':\n",
    "        hots.append(x[i][15:-1])\n",
    "\n",
    "hots_df = pd.DataFrame (hots, columns = ['episode_name'])\n",
    "hots_df['cold_open'] = np.zeros(12).astype(int)\n",
    "hots_df = hots_df.replace(to_replace = 'Weight Loss', value = 'Weight Loss (Parts 1&2)')\n",
    "data_ = pd.merge(data, hots_df, on='episode_name', how='left')\n",
    "data_['cold_open'] = data_['cold_open'].fillna(1)\n",
    "\n",
    "data_.loc[data_['cold_open'] == 1]\n",
    "\n",
    "data = data_\n",
    "data_explore = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making sure that all the necessary libraries or submodules are uploaded here, please follow the given skeleton to create your project report. \n",
    "- Your completed assignment must follow this structure \n",
    "- You should not add or remove any of these sections, if you feel it is necessary you may add extra subsections within each (such as *2.1. Encoding*). \n",
    "\n",
    "**Do not forget to remove the instructions for each section in the final document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K314dGEL7Fo1"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "*This section should include a brief introduction to the task and the data (assume this is a report you are delivering to a client).* \n",
    "\n",
    "- If you use any additional data sources, you should introduce them here and discuss why they were included.\n",
    "\n",
    "- Briefly outline the approaches being used and the conclusions that you are able to draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P6Vdzbo7Fo2"
   },
   "source": [
    "## 2. Exploratory Data Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arg9_dYE7Fo2"
   },
   "source": [
    "*Include a detailed discussion of the data with a particular emphasis on the features of the data that are relevant for the subsequent modeling.* \n",
    "\n",
    "- Including visualizations of the data is strongly encouraged - all code and plots must also be described in the write up. \n",
    "- Think carefully about whether each plot needs to be included in your final draft - your report should include figures but they should be as focused and impactful as possible.\n",
    "\n",
    "*Additionally, this section should also implement and describe any preprocessing / feature engineering of the data.*\n",
    "\n",
    "- Specifically, this should be any code that you use to generate new columns in the data frame `d`. All of this processing is explicitly meant to occur before we split the data in to training and testing subsets. \n",
    "- Processing that will be performed as part of an sklearn pipeline can be mentioned here but should be implemented in the following section.*\n",
    "\n",
    "**All code and figures should be accompanied by text that provides an overview / context to what is being done or presented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Columns in the df are:\")\n",
    "print (data_explore.columns.tolist(), \"\\n\")\n",
    "\n",
    "dup_num = pd.DataFrame(data_explore).duplicated().sum()\n",
    "print(\"There are {} duplicated values in the data\".format(dup_num))\n",
    "\n",
    "na_num = data_explore.isna().sum().sum()\n",
    "print(f\"There are {na_num} Na values in the data\")\n",
    "\n",
    "data_explore.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the correlation between the current numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (10, 6)})\n",
    "sns.heatmap(data_explore.corr(), annot = True, fmt = '.2f', linewidths = 2)\n",
    "plt.title(\"Correlation Heatmap for current numerical variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first drop the episode name and episode number features because they are not relevant to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_explore = data_explore.drop(\"episode_name\", axis=1)\n",
    "data_explore = data_explore.drop(\"episode\", axis=1)\n",
    "\n",
    "print (data_explore.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the variable \"air_date\" to create two new features: Categorical variable \"calendar_season\", being the season (autumn, winter, spring, summer) when the episode was aired; and (numerical variable?) \"air_month\" being the month the episode aired. After this we drop \"air_date\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['air_date'] = pd.to_datetime(data['air_date'])\n",
    "\n",
    "# convert the 'dates' column to datetime format\n",
    "data_explore['air_date'] = pd.to_datetime(data_explore['air_date'])\n",
    "\n",
    "# create a new categorical column 'calendar_season' by assigning a category \"winter\", \n",
    "# \"spring\", \"summer\", \"autumn\" depending on the air-month.\n",
    "data_explore['calendar_season'] = pd.cut(\n",
    "    data_explore['air_date'].dt.month, \n",
    "    [0, 3, 6, 9, 12], \n",
    "    labels=['winter', 'spring', 'summer', 'autumn']\n",
    ")\n",
    "\n",
    "# convert calender season to string column rather than categorical column.\n",
    "data_explore['calendar_season'] = data_explore['calendar_season'].astype(str)\n",
    "\n",
    "# Create month column.\n",
    "data_explore['month'] = data_explore['air_date'].dt.month\n",
    "\n",
    "data_explore.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_explore = data_explore.drop(\"air_date\", axis=1)\n",
    "data_explore.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a one hot encoder function to assign a binary attribute per category for the Categorical variables (\"director, \"writer\", \"calendar_season\" and \"main_chars\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHot_encoder(df, cat_variable):\n",
    "    oneHot_df = df.copy()\n",
    "    variable_initials = ''.join([word[0] for word in cat_variable.split(\"_\")])\n",
    "\n",
    "    \n",
    "    unique_chars = set(';'.join(oneHot_df[cat_variable]).split(';'))\n",
    "\n",
    "    # Create binary attributes for each unique character\n",
    "    for char in unique_chars:\n",
    "        oneHot_df[variable_initials+\"_\"+char] = oneHot_df[cat_variable].apply(lambda x: 1 if char in x else 0)\n",
    "    \n",
    "    \n",
    "    # Drop the original main_chars column\n",
    "    oneHot_df = oneHot_df.drop(columns=[cat_variable])\n",
    "    \n",
    "    return oneHot_df\n",
    "\n",
    "\n",
    "cat_variables = ['director', 'writer', 'main_chars', 'calendar_season']\n",
    "\n",
    "for cat_variable in cat_variables:\n",
    "    data_explore = OneHot_encoder(data_explore, cat_variable)\n",
    "    \n",
    "\n",
    "data_explore.head(5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection we realised that there are some repeated directors where in some data entries the name of the director is spelled wrong. The specific cases were:\n",
    "\n",
    " - Charles McDougall (correct name) and Charles McDougal\n",
    " - Claire Scanlon (correct name) and Claire Scanlong \n",
    " - Greg Daniels (correct name) and Greg Daneils\n",
    " - Ken Whittingham (correct name) and Ken Wittingham \n",
    " - Paul Lieberstein (correct name) and Paul Lieerstein\n",
    " \n",
    "Thus we now write code to remove columns with the wrong name, where we will put a 1 in the correct name column if the incorrect one had a 1 and the correct one didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'d_Charles McDougal'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'd_Charles McDougal'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1348\\104851003.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdata_explore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_explore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrong_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdata_explore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_explore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrong_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'd_Charles McDougal'"
     ]
    }
   ],
   "source": [
    "correct_names = ['d_Charles McDougall','d_Claire Scanlon','d_Greg Daniels', 'd_Ken Whittingham', 'd_Paul Lieberstein']\n",
    "wrong_names = ['d_Charles McDougal','d_Claire Scanlong','d_Greg Daneils', 'd_Ken Wittingham', 'd_Paul Lieerstein']\n",
    "\n",
    "for i, correct_name in enumerate(correct_names):\n",
    "    data_explore.loc[data_explore[wrong_names[i]] == 1, correct_name] = 1\n",
    "    data_explore = data_explore.drop(wrong_names[i], axis=1)\n",
    "    \n",
    "data_explore.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding some further data to the model\n",
    "In the following cells we add some data to the model from Nasir Khalid's dataset (Khalid, 2021). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>title</th>\n",
       "      <th>scene</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>Michael</td>\n",
       "      <td>All right Jim. Your quarterlies look very good...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>Jim</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>Michael</td>\n",
       "      <td>So you've come to the master for guidance? Is ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>Jim</td>\n",
       "      <td>Actually, you called me in here, but yeah.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>Michael</td>\n",
       "      <td>All right. Well, let me show you how it's done.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  episode  title  scene  speaker  \\\n",
       "0       1        1  Pilot      1  Michael   \n",
       "1       1        1  Pilot      1      Jim   \n",
       "2       1        1  Pilot      1  Michael   \n",
       "3       1        1  Pilot      1      Jim   \n",
       "4       1        1  Pilot      1  Michael   \n",
       "\n",
       "                                                line Unnamed: 6  \n",
       "0  All right Jim. Your quarterlies look very good...        NaN  \n",
       "1         Oh, I told you. I couldn't close it. So...        NaN  \n",
       "2  So you've come to the master for guidance? Is ...        NaN  \n",
       "3         Actually, you called me in here, but yeah.        NaN  \n",
       "4    All right. Well, let me show you how it's done.        NaN  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.read_csv(\"The-Office-Lines-V4.csv\").copy()\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset, as seen above, contains each line in the show, demarcated by speaker and scene. In the cell bellow, we aggregate the lines per character per episode, to give the total lines spoken by each character in each episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode_code</th>\n",
       "      <th>speaker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">1.1</th>\n",
       "      <th>Angela</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dwight</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>693</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jan</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>120</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>982</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Man</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michael</th>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>1594</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michel</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oscar</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pam</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1099</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phyllis</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roy</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ryan</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>211</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stanley</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Todd Packer</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1.2</th>\n",
       "      <th>Angela</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>252</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dwight</th>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>860</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim</th>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>1356</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kelly</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>473</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          season  episode  scene  dummy\n",
       "episode_code speaker                                   \n",
       "1.1          Angela            1        1     14      1\n",
       "             Dwight           29       29    693     29\n",
       "             Jan              12       12    120     12\n",
       "             Jim              36       36    982     36\n",
       "             Kevin             1        1     14      1\n",
       "             Man               1        1     26      1\n",
       "             Michael          81       81   1594     81\n",
       "             Michel            1        1     11      1\n",
       "             Oscar             3        3     63      3\n",
       "             Pam              40       40   1099     40\n",
       "             Phyllis           2        2     38      2\n",
       "             Roy               5        5    171      5\n",
       "             Ryan              8        8    211      8\n",
       "             Stanley           5        5    116      5\n",
       "             Todd Packer       3        3     33      3\n",
       "1.2          Angela            4        8    252      4\n",
       "             Dwight           17       34    860     17\n",
       "             Jim              25       50   1356     25\n",
       "             Kelly             2        4    121      2\n",
       "             Kevin             8       16    473      8"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a list of unique speakers from the speaker column\n",
    "unique_speakers = new_data.speaker.unique()\n",
    "\n",
    "#Create a dummy column which tracks episode, since season:episode are two columns\n",
    "new_data[\"episode_code\"] = new_data.season.astype(str)+\".\"+new_data.episode.astype(str)\n",
    "\n",
    "#Add a dummy 1 to each line, so that the sum will count totals per character for each line\n",
    "new_data[\"dummy\"] = 1\n",
    "\n",
    "new_data.head()\n",
    "\n",
    "#By episode and speaker, sum the total number of lines\n",
    "lines = new_data.groupby([\"episode_code\",\"speaker\"]).sum()\n",
    "\n",
    "#Need to now take the dummy sums and probably take the max (most-speaker) in an episode and maybe second most\n",
    "#e.g. episode 1.1 has michael 81 lines and pam 40 lines so take 1.1 to have \"most speech\"=\"michael\" and \"second most\"=\"pam\"\n",
    "lines.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad7J-Yw67Fo3"
   },
   "source": [
    "## 3. Model Fitting and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXdvOaZs7Fo3"
   },
   "source": [
    "*In this section you should detail your choice of model and describe the process used to refine and fit that model.*\n",
    "\n",
    "- You are strongly encouraged to explore many different modeling methods (e.g. linear regression, regression trees, lasso, etc.) but you should not include a detailed narrative of all of these attempts. \n",
    "- At most this section should mention the methods explored and why they were rejected - most of your effort should go into describing the model you are using and your process for tuning and validatin it.\n",
    "\n",
    "*For example if you considered a linear regression model, a classification tree, and a lasso model and ultimately settled on the linear regression approach then you should mention that other two approaches were tried but do not include any of the code or any in depth discussion of these models beyond why they were rejected. This section should then detail is the development of the linear regression model in terms of features used, interactions considered, and any additional tuning and validation which ultimately led to your final model.* \n",
    "\n",
    "**This section should also include the full implementation of your final model, including all necessary validation. As with figures, any included code must also be addressed in the text of the document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edQVULLU7Fo3"
   },
   "source": [
    "## 4. Discussion and Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi0yKPrQ7Fo4"
   },
   "source": [
    "*In this section you should provide a general overview of **your final model**, its **performance**, and **reliability**.* \n",
    "\n",
    "- You should discuss what the implications of your model are in terms of the included features, predictive performance, and anything else you think is relevant.\n",
    "\n",
    "- This should be written with a target audience of a NBC Universal executive who is with the show and university level mathematics but not necessarily someone who has taken a postgraduate statistical modeling course. \n",
    "\n",
    "- Your goal should be to convince this audience that your model is both accurate and useful.\n",
    "\n",
    "- Finally, you should include concrete recommendations on what NBC Universal should do to make their reunion episode a popular as possible.\n",
    "\n",
    "**Keep in mind that a negative result, i.e. a model that does not work well predictively, but that is well explained and justified in terms of why it failed will likely receive higher marks than a model with strong predictive performance but with poor or incorrect explanations / justifications.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References\n",
    "\n",
    "*In this section, you should present a list of external sources (except the course materials) that you used during the project, if any*\n",
    "\n",
    "Additional data sources can be cited here, in addition to related python documentations, any other webpage sources that you benefited from\n",
    "\n",
    "* Khalid, Nasir. 2021. *The Office (US) - Complete Dialogue/Transcript*, Kaggle. Accessed on 04/03/2023. URL: https://www.kaggle.com/datasets/nehaprabhavalkar/the-office-dataset"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
