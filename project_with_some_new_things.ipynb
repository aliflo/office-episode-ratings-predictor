{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOyOkJJA7Foy"
   },
   "source": [
    "# Predicting The Office episode ratings to advise NBC Universal on a reunion episode\n",
    "#### Machine Learning in Python - Group Project 1\n",
    "*Data Scientist Contractors: Luke Appleby, Josep Reyero, Tom Birkbeck, Adam Dunajski*\n",
    "\n",
    "In this report, we explore what makes a great episode of The Office, NBC Universal's widely-regarded situational comedy about the office life of a paper manufacturer. \n",
    "\n",
    "This is achieved through a machine learning model which predicts the rating of the episode based on a number of episode characteristics - such as whether the episode features a cold open, and how many spoken lines are in it. \n",
    "\n",
    "We use this model to give a reccomendation on episode features which will **maximise IMBD rating** for the episode.\n",
    "\n",
    "The report is formatted in the style of an interactive workbook in which the client (NBC Universal) can step through cells of code interspersed with explanation of model choices and mathematical background.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "0.  Predicting The Office episode ratings to advise NBC Universal on a reunion episode\n",
    "1. Introduction    \n",
    "    1.1 Model and conclusions overview/precis    \n",
    "    1.2 Data sources used    \n",
    "    1.3 General code setup       \n",
    "2. Exploratory Data Analysis and Feature Engineering    \n",
    "    2.1 **<span style=\"color:red\">SECTION</span>**    \n",
    "    2.2 **<span style=\"color:red\">SECTION</span>**     \n",
    "    2.3 **<span style=\"color:red\">SECTION</span>**    \n",
    "    2.4 **<span style=\"color:red\">SECTION</span>**    \n",
    "3. Model fitting and tuning    \n",
    "    3.1 **<span style=\"color:red\">SECTION</span>**    \n",
    "    3.2 **<span style=\"color:red\">SECTION</span>**    \n",
    "    3.3 **<span style=\"color:red\">SECTION</span>**    \n",
    "4. Discussion and Conclusions    \n",
    "    4.1 **<span style=\"color:red\">SECTION</span>**    \n",
    "    4.2 **<span style=\"color:red\">SECTION</span>**    \n",
    "    4.3 **<span style=\"color:red\">SECTION</span>**    \n",
    "5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "In this section, we give an abstract of our modelling approach and truncated results, intended for senior managers who are short on time to view. \n",
    "\n",
    "We then discuss our data sources and set-up of the code environment for the model.\n",
    "\n",
    "## 1.1 Model and conclusions overview/precis\n",
    "\n",
    "After comparing a number of models, we found a **<span style=\"color:red\">[WHICH MODEL DID WE FIND BEST]</span>** model was the best to fit to the data. It had a prediction accuracy of **<span style=\"color:red\">[WHAT WAS ITS ACCURACY]</span>**, which is very high.\n",
    "\n",
    "The model indicates that strong postive correlants of episodic IMBD rating are **<span style=\"color:red\">[WHAT WERE THE CORRELANTS]</span>**, therefore, we advise NBC Universal to apply the following list of reccomendations when writing the reunion episode.\n",
    "\n",
    "\n",
    "<span style=\"color:red\">[WHAT WERE THE CORRELANTS]\n",
    "* COLD OPEN\n",
    "* OTHER THINGS\n",
    "    </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data sources\n",
    "Our primary data source was the office episode ratings dataset the_office.csv (Evkaya, 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPpPRE37Fo0"
   },
   "source": [
    "## 1.3 General Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jb4qh-pi7Fo1"
   },
   "outputs": [],
   "source": [
    "# Add any additional libraries or submodules below\n",
    "\n",
    "# Data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules that are necessary\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hjs5785u7Fo1"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"the_office.csv\")\n",
    "data_explore = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MLP_cold_opens.txt', 'r') as file:\n",
    "    new_data = file.read().replace('\\n', '')\n",
    "\n",
    "x = new_data.split(\")\")\n",
    "hots = []\n",
    "for i in range(len(x)):\n",
    "    if x[i][0:4] == 'None':\n",
    "        hots.append(x[i][7:-1])\n",
    "    elif x[i][0:8] == 'Season 1' and x[i][8:12] == 'None':\n",
    "        hots.append(x[i][14:])\n",
    "    elif x[i][0:6] == 'Season' and x[i][8:12] == 'None':\n",
    "        hots.append(x[i][15:-1])\n",
    "\n",
    "hots_df = pd.DataFrame (hots, columns = ['episode_name'])\n",
    "hots_df['cold_open'] = np.zeros(12).astype(int)\n",
    "hots_df = hots_df.replace(to_replace = 'Weight Loss', value = 'Weight Loss (Parts 1&2)')\n",
    "data_ = pd.merge(data, hots_df, on='episode_name', how='left')\n",
    "data_['cold_open'] = data_['cold_open'].fillna(1)\n",
    "\n",
    "data_.loc[data_['cold_open'] == 1]\n",
    "\n",
    "data = data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MLP_jim_pranks.txt', 'r') as file:\n",
    "    pranks_str = file.read().replace('\\n', '')\n",
    "    \n",
    "x0 = pranks_str.split('(')\n",
    "j = 0\n",
    "for i in x0:\n",
    "    x0[j] = i.split(')')\n",
    "    j+=1\n",
    "j = 0\n",
    "for i in x0:\n",
    "    x0[j] = i[0]\n",
    "    j +=1\n",
    "for i in range(len(x0)):\n",
    "    x0[i] = x0[i][1:-1]\n",
    "\n",
    "\n",
    "names = data['episode_name']\n",
    "\n",
    "names = names.replace(to_replace = 'E-Mail Surveilance', value = 'Email Surveillance')\n",
    "names = names.replace(to_replace = 'A Benihana Christmas ', value = 'A Benihana Christmas')\n",
    "names = names.replace(to_replace = 'The Job ', value = 'The Job', regex = True)\n",
    "names = names.replace(to_replace = 'Launch Party ', value = 'Launch Party', regex = True)\n",
    "names = names.replace(to_replace = 'Goodbye, Toby ', value = 'Goodbye Toby', regex = True)\n",
    "names = names.replace(to_replace = 'Niagara ', value = 'Niagara', regex = True)\n",
    "names = names.replace(to_replace = 'The Cover', value = 'The Cover-Up', regex = True)\n",
    "names = names.replace(to_replace = 'Classy Christmas ', value = 'Classy Christmas', regex = True)\n",
    "names = names.replace(to_replace = 'Dwight K. Schrute, (Acting) Manager', value = 'Dwight K. Schrute,')\n",
    "names = np.array(names)\n",
    "\n",
    "j = 0\n",
    "for i in names:\n",
    "    if i[-11:] == '(Parts 1&2)':\n",
    "        names[j] = i[:-11]\n",
    "    elif i[-8:] == '(Part 1)' or i[-8:] == '(Part 2)':\n",
    "        names[j] = i[:-8]\n",
    "    j +=1\n",
    "        \n",
    "pranks = []\n",
    "for i in names:\n",
    "    #print(x0.count(i))\n",
    "    pranks.append(x0.count(i))\n",
    "#     a += x0.count(i)\n",
    "\n",
    "data['jim_pranks'] = pranks\n",
    "\n",
    "data\n",
    "data_explore = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'jim_and_pam_key_episodes.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-694363e3a2e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'jim_and_pam_key_episodes.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpam_jim_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'jim_and_pam_key_episodes.txt'"
     ]
    }
   ],
   "source": [
    "with open('jim_and_pam_key_episodes.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "start = 0\n",
    "pam_jim_episodes = []\n",
    "\n",
    "while True:\n",
    "    start = text.find('(', start) + 1\n",
    "    if start == 0:\n",
    "        break\n",
    "        \n",
    "    end = text.find(')', start)\n",
    "    between_brackets = text[start:end]\n",
    "    \n",
    "    if any(char.isdigit() for char in between_brackets):\n",
    "        season = int(between_brackets[1])\n",
    "        after_E_string = between_brackets[between_brackets.find('E')+1:]\n",
    "        \n",
    "        if '/' in after_E_string:\n",
    "            pam_jim_episodes.append([season, int(after_E_string[:after_E_string.find('/')])])\n",
    "            pam_jim_episodes.append([season, int(after_E_string[after_E_string.find('/')+1:])])\n",
    "        \n",
    "        else:\n",
    "            pam_jim_episodes.append([season, int(after_E_string)])\n",
    "        \n",
    "    start = end + 1\n",
    "    \n",
    "    \n",
    "def is_key_episode(row, key_episodes):\n",
    "    season, episode = row['season'], row['episode']\n",
    "    return int([season, episode] in key_episodes)\n",
    "\n",
    "data_explore['pam_and_jim_key_episode'] = data_explore.apply(\n",
    "    lambda row: is_key_episode(row, pam_jim_episodes), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K314dGEL7Fo1"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "*This section should include a brief introduction to the task and the data (assume this is a report you are delivering to a client).* \n",
    "\n",
    "- If you use any additional data sources, you should introduce them here and discuss why they were included.\n",
    "\n",
    "- Briefly outline the approaches being used and the conclusions that you are able to draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P6Vdzbo7Fo2"
   },
   "source": [
    "## 2. Exploratory Data Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arg9_dYE7Fo2"
   },
   "source": [
    "*Include a detailed discussion of the data with a particular emphasis on the features of the data that are relevant for the subsequent modeling.* \n",
    "\n",
    "- Including visualizations of the data is strongly encouraged - all code and plots must also be described in the write up. \n",
    "- Think carefully about whether each plot needs to be included in your final draft - your report should include figures but they should be as focused and impactful as possible.\n",
    "\n",
    "*Additionally, this section should also implement and describe any preprocessing / feature engineering of the data.*\n",
    "\n",
    "- Specifically, this should be any code that you use to generate new columns in the data frame `d`. All of this processing is explicitly meant to occur before we split the data in to training and testing subsets. \n",
    "- Processing that will be performed as part of an sklearn pipeline can be mentioned here but should be implemented in the following section.*\n",
    "\n",
    "**All code and figures should be accompanied by text that provides an overview / context to what is being done or presented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Columns in the df are:\")\n",
    "print (data_explore.columns.tolist(), \"\\n\")\n",
    "\n",
    "dup_num = pd.DataFrame(data_explore).duplicated().sum()\n",
    "print(\"There are {} duplicated values in the data\".format(dup_num))\n",
    "\n",
    "na_num = data_explore.isna().sum().sum()\n",
    "print(f\"There are {na_num} Na values in the data\")\n",
    "\n",
    "data_explore.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the correlation between the current numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (10, 6)})\n",
    "sns.heatmap(data_explore.corr(), annot = True, fmt = '.2f', linewidths = 2)\n",
    "plt.title(\"Correlation Heatmap for current numerical variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first drop the episode name and episode number features because they are not relevant to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_explore = data_explore.drop(\"episode_name\", axis=1)\n",
    "data_explore = data_explore.drop(\"episode\", axis=1)\n",
    "\n",
    "print (data_explore.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the variable \"air_date\" to create two new features: Categorical variable \"calendar_season\", being the season (autumn, winter, spring, summer) when the episode was aired; and (numerical variable?) \"air_month\" being the month the episode aired. After this we drop \"air_date\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['air_date'] = pd.to_datetime(data['air_date'])\n",
    "\n",
    "# convert the 'dates' column to datetime format\n",
    "data_explore['air_date'] = pd.to_datetime(data_explore['air_date'])\n",
    "\n",
    "# create a new categorical column 'calendar_season' by assigning a category \"winter\", \n",
    "# \"spring\", \"summer\", \"autumn\" depending on the air-month.\n",
    "data_explore['calendar_season'] = pd.cut(\n",
    "    data_explore['air_date'].dt.month, \n",
    "    [0, 3, 6, 9, 12], \n",
    "    labels=['winter', 'spring', 'summer', 'autumn']\n",
    ")\n",
    "\n",
    "# convert calender season to string column rather than categorical column.\n",
    "data_explore['calendar_season'] = data_explore['calendar_season'].astype(str)\n",
    "\n",
    "# Create month column.\n",
    "data_explore['month'] = data_explore['air_date'].dt.month\n",
    "\n",
    "data_explore.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_explore = data_explore.drop(\"air_date\", axis=1)\n",
    "data_explore.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_explore.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a one hot encoder function to assign a binary attribute per category for the Categorical variables (\"director, \"writer\", \"calendar_season\" and \"main_chars\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHot_encoder(df, cat_variable):\n",
    "    oneHot_df = df.copy()\n",
    "    variable_initials = ''.join([word[0] for word in cat_variable.split(\"_\")])\n",
    "\n",
    "    \n",
    "    unique_chars = set(';'.join(oneHot_df[cat_variable]).split(';'))\n",
    "\n",
    "    # Create binary attributes for each unique character\n",
    "    for char in unique_chars:\n",
    "        oneHot_df[variable_initials+\"_\"+char] = oneHot_df[cat_variable].apply(lambda x: 1 if char in x else 0)\n",
    "    \n",
    "    \n",
    "    # Drop the original main_chars column\n",
    "    oneHot_df = oneHot_df.drop(columns=[cat_variable])\n",
    "    \n",
    "    return oneHot_df\n",
    "\n",
    "\n",
    "cat_variables = ['director', 'writer', 'main_chars', 'calendar_season']\n",
    "\n",
    "for cat_variable in cat_variables:\n",
    "    data_explore = OneHot_encoder(data_explore, cat_variable)\n",
    "    \n",
    "\n",
    "data_explore.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data_explore.columns)\n",
    "mc_list = [columns[i] for i in range(len(columns)) if columns[i][:3] == \"mc_\"]\n",
    "n = len(mc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairnames = []\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        pairname = \"pair_\" + mc_list[i][3:] + \"_\"+ mc_list[j][3:]\n",
    "        pairnames.append(pairname)\n",
    "        data_explore[pairname] = ((data_explore[mc_list[i]] + data_explore[mc_list[j]])//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = data_explore.corr()[\"imdb_rating\"]\n",
    "\n",
    "corr_array = np.zeros((n,n))\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        pairname = \"pair_\" + mc_list[i][3:] + \"_\"+ mc_list[j][3:]\n",
    "        corr_array[i][j] = corr_mat[pairname]\n",
    "        corr_array[j][i] = corr_mat[pairname]\n",
    "    corr_array[i][i] = corr_mat[mc_list[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (10,10), ncols = 1)\n",
    "\n",
    "mc_names = [mc_name[3:] for mc_name in mc_list]\n",
    "plt.grid(b=False)\n",
    "plt.imshow(corr_array)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(n), mc_names, rotation ='vertical')\n",
    "plt.yticks(range(n), mc_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection we realised that there are some repeated directors where in some data entries the name of the director is spelled wrong. The specific cases were:\n",
    "\n",
    " - Charles McDougall (correct name) and Charles McDougal\n",
    " - Claire Scanlon (correct name) and Claire Scanlong \n",
    " - Greg Daniels (correct name) and Greg Daneils\n",
    " - Ken Whittingham (correct name) and Ken Wittingham \n",
    " - Paul Lieberstein (correct name) and Paul Lieerstein\n",
    " \n",
    "Thus we now write code to remove columns with the wrong name, where we will put a 1 in the correct name column if the incorrect one had a 1 and the correct one didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_names = ['d_Charles McDougall','d_Claire Scanlon','d_Greg Daniels', 'd_Ken Whittingham', 'd_Paul Lieberstein']\n",
    "wrong_names = ['d_Charles McDougal','d_Claire Scanlong','d_Greg Daneils', 'd_Ken Wittingham', 'd_Paul Lieerstein']\n",
    "\n",
    "for i, correct_name in enumerate(correct_names):\n",
    "    data_explore.loc[data_explore[wrong_names[i]] == 1, correct_name] = 1\n",
    "    data_explore = data_explore.drop(wrong_names[i], axis=1)\n",
    "    \n",
    "data_explore.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_explore.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad7J-Yw67Fo3"
   },
   "source": [
    "## 3. Model Fitting and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXdvOaZs7Fo3"
   },
   "source": [
    "*In this section you should detail your choice of model and describe the process used to refine and fit that model.*\n",
    "\n",
    "- You are strongly encouraged to explore many different modeling methods (e.g. linear regression, regression trees, lasso, etc.) but you should not include a detailed narrative of all of these attempts. \n",
    "- At most this section should mention the methods explored and why they were rejected - most of your effort should go into describing the model you are using and your process for tuning and validatin it.\n",
    "\n",
    "*For example if you considered a linear regression model, a classification tree, and a lasso model and ultimately settled on the linear regression approach then you should mention that other two approaches were tried but do not include any of the code or any in depth discussion of these models beyond why they were rejected. This section should then detail is the development of the linear regression model in terms of features used, interactions considered, and any additional tuning and validation which ultimately led to your final model.* \n",
    "\n",
    "**This section should also include the full implementation of your final model, including all necessary validation. As with figures, any included code must also be addressed in the text of the document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Having done all the data exploration and feature engineering, we not set our starting dataframe to the \n",
    "# engineered dataframe.\n",
    "data = data_explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by fitting a multiple linear regression, and testing performance. We use a sklearn pipeline to scale and fit the model.  features to have a mean of zero and a standard deviation of one. We will sue standardScaler to ensure that all features have equal importance when fitting a linear regression model, and can improve the stability and convergence of the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop('imdb_rating', axis=1), data['imdb_rating'], \n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "\n",
    "# Fitting the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model\n",
    "y_predict = model_pipeline.predict(X_test)\n",
    "\n",
    "# Printing performance metrics\n",
    "print(f\"Mean squared error: {mean_squared_error(y_test, y_predict)}\")\n",
    "print(f\"R-squared: {r2_score(y_test, y_predict)}\")\n",
    "\n",
    "sns.scatterplot(x=y_test, y=y_predict)\n",
    "plt.xlabel('Actual IMDB rating')\n",
    "plt.ylabel('Predicted IMDB rating')\n",
    "plt.title('Linear regression model performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edQVULLU7Fo3"
   },
   "source": [
    "## 4. Discussion and Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi0yKPrQ7Fo4"
   },
   "source": [
    "*In this section you should provide a general overview of **your final model**, its **performance**, and **reliability**.* \n",
    "\n",
    "- You should discuss what the implications of your model are in terms of the included features, predictive performance, and anything else you think is relevant.\n",
    "\n",
    "- This should be written with a target audience of a NBC Universal executive who is with the show and university level mathematics but not necessarily someone who has taken a postgraduate statistical modeling course. \n",
    "\n",
    "- Your goal should be to convince this audience that your model is both accurate and useful.\n",
    "\n",
    "- Finally, you should include concrete recommendations on what NBC Universal should do to make their reunion episode a popular as possible.\n",
    "\n",
    "**Keep in mind that a negative result, i.e. a model that does not work well predictively, but that is well explained and justified in terms of why it failed will likely receive higher marks than a model with strong predictive performance but with poor or incorrect explanations / justifications.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References\n",
    "\n",
    "*In this section, you should present a list of external sources (except the course materials) that you used during the project, if any*\n",
    "\n",
    "Additional data sources can be cited here, in addition to related python documentations, any other webpage sources that you benefited from\n",
    "\n",
    "\n",
    "* Evkaya, Ozan. 2023. *The Office CSV Data File*, MATH11205: Machine Learning in Python, The University of Edinburgh."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
